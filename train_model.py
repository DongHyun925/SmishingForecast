import os
import json
import torch
import sys
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.detector import SmishingDetector

# --- ì„¤ì • ---
EPOCHS = 3
BATCH_SIZE = 8
LEARNING_RATE = 2e-5
SAVE_PATH = "models/smishing_detector_model.pth"

class SmishingDataset(Dataset):
    def __init__(self, data, tokenizer, max_len=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']
        label = item['label']

        encoding = self.tokenizer(
            text,
            return_tensors='pt',
            max_length=self.max_len,
            padding='max_length',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def train():
    print("=== ğŸš€ ëª¨ë¸ í•™ìŠµ (Fine-tuning) ì‹œì‘ ===")
    
    # 1. ë°ì´í„° ë¡œë“œ
    data_path = os.path.join("data", "train_dataset.json")
    if not os.path.exists(data_path):
        print(f"[!] í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return
        
    with open(data_path, "r", encoding="utf-8") as f:
        train_data = json.load(f)
        
    print(f"[*] í•™ìŠµ ë°ì´í„°: {len(train_data)}ê±´ ë¡œë“œ ì™„ë£Œ")

    # 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    # detectorë¥¼ ì´ˆê¸°í™”í•˜ë©´ pre-trained ëª¨ë¸ì´ ë¡œë“œë¨
    detector = SmishingDetector() 
    model = detector.model
    tokenizer = detector.tokenizer
    device = detector.device
    
    model.train() # í•™ìŠµ ëª¨ë“œ ì „í™˜

    # 3. DataLoader ì¤€ë¹„
    dataset = SmishingDataset(train_data, tokenizer)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

    # 4. í•™ìŠµ ë£¨í”„
    print(f"[*] í•™ìŠµ ì‹œì‘ (Epochs: {EPOCHS}, Device: {device})")
    print("-" * 50)
    
    for epoch in range(EPOCHS):
        total_loss = 0
        loop = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        
        for batch in loop:
            optimizer.zero_grad()
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            loop.set_postfix(loss=loss.item())
            
        avg_loss = total_loss / len(dataloader)
        print(f"    -> Epoch {epoch+1} Average Loss: {avg_loss:.4f}")

    # 5. ëª¨ë¸ ì €ì¥
    os.makedirs("models", exist_ok=True)
    
    # ì „ì²´ ëª¨ë¸(ê°€ì¤‘ì¹˜) ì €ì¥
    torch.save(model.state_dict(), SAVE_PATH)
    print("=" * 50)
    print(f"[Success] í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")
    print("ì´ì œ eval_pure_model.pyë¥¼ ì‹¤í–‰í•˜ë©´ ì„±ëŠ¥ì´ ëŒ€í­ í–¥ìƒë˜ì—ˆì„ ê²ƒì…ë‹ˆë‹¤!")

if __name__ == "__main__":
    train()
